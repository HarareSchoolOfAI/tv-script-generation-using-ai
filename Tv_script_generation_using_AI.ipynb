{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eq0Y1zqgPdJa"
   },
   "source": [
    "# Using AI to write a script for a TV show\n",
    "\n",
    "### So what are we doing?\n",
    "- Well, we are going to be using AI to write a manuscript for a television show called <b>The Simpsons</b> for us!\n",
    "\n",
    "### Why?\n",
    "- Because it's cool, and because we are <b>AWESOME</b>! And that's why.\n",
    "- Also, it helps us to understand the other applications of <b>N</b>atural <b>L</b>anguage <b>P</b>rocessing other than <b>Chatbots</b>. \n",
    "\n",
    "### How are we going to be doing it?\n",
    "- We are going to be defining a neural network that will do it for us! (Sounds <b>unbelievable</b> right? I know)\n",
    "- Using text generation, we want to see how well <b>AI</b> can  write more Simpsons scripts based on 600 episodes of text as training data.\n",
    "\n",
    "\n",
    "#### So  we are going to start with imports down here: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4-N35flDyFqF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we are done with imports, we can get straight to business! But before that, we need to define what business we have first and what we are going to use for the business.\n",
    "- So, in order for us to train our model, we need the dataset to be loaded into memory. The dataset we are going to use is the script lines from <b>The Simpsons</b> show. (All 600 episodes compressed into one text file! Can you imagine?)\n",
    "- After loading the dataset, we need to get unique characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "text",
    "id": "b928mMuOTUmn"
   },
   "outputs": [],
   "source": [
    "dataset_filename = 'the-simpsons-dataset.txt'\n",
    "\n",
    "# open the file and read it\n",
    "raw_text = open(dataset_filename).read()\n",
    "\n",
    "# we have to lower the data to make sure our network recognises all characters. \n",
    "#Also it makes one hot encoding easier\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "unwanted_text = '/:;][{}-=+)(*&^%-_$#@!~1234567890'\n",
    "\n",
    "# get rid of noise! If you hate noise raise your hand and say 'Python!'\n",
    "cleaned_text = raw_text.translate(unwanted_text)\n",
    "\n",
    "# First let's get the unique characters\n",
    "characters = sorted(list(set(cleaned_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we got this far, it means we now have a clean dataset loaded into memory. <b>Cool</b> right?\n",
    "- Then <b>what</b>? Then we convert the characters to integers.\n",
    "- I hope you are not asking <b>why</b>?! Because if you are, the answer is, at the moment it's impossible to model the characters directly (Computers understand numbers more than 'Hello, my name is _whatshisname_ '). So they have to go into an intermediary phase, where they are converted into numbers(specifically integers)....\n",
    "- We omit uncommon symbols (_which is the_ <b>`unwanted_text` </b> _variable_) because of the noise that they add to the data, and omit spaces in consideration of the size-related problems we might face during training, after all, spaces add a considerable amount of size to the data. \n",
    "- With these features filtered out of the data, and the data now in a sorted list form, we then create two dictionaries allowing us to translate between word IDs and the words that they represent. The word IDs allow the words to be represented numerically in the model, and the mapping between words and their IDs allows the model's output to be readable by the ~~earth's most abundant plag~~ humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 11027541\n",
      "The length of the vocabulary: 107\n"
     ]
    }
   ],
   "source": [
    "# Now we start mapping!\n",
    "character_to_integer = dict((c, i) for i, c in enumerate(characters))\n",
    "integer_to_character = dict((i, c) for i, c in enumerate(characters))\n",
    "\n",
    "# Summary of what we now have after that sort of tedious process\n",
    "num_characters = len(cleaned_text)\n",
    "vocabulary_length = len(characters)\n",
    "\n",
    "print('Total number of characters: {}'.format(num_characters))\n",
    "print('The length of the vocabulary: {}'.format(vocabulary_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now for those who might have been wondering, 'Oh no! What was that all about?', allow me to briefly explain in short (_very short_).\n",
    "- Take this as example, the list of unique sorted lowercase characters in the dataset could be as follows:\n",
    "\n",
    "<code>\n",
    "['\\n', '\\r', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xbb', '\\xbf', '\\xef']\n",
    "</code>\n",
    "\n",
    "- You can see that there may be some characters that we could remove to further clean up the dataset that will reduce the vocabulary and may improve the modeling process. (Which is what we did at the start!)\n",
    "\n",
    "- After cleaning, we created a set of all of the distinct characters in the dataset, then creating a map of each character to a unique integer.\n",
    "- Then finally, (something that looks serious), we summarized number of unique characters and the length of the vocabulary that we are going to be using for training (in a few minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now need to define the training data for the network. There is a lot of flexibility in how you choose to break up the text and expose it to the network during training.\n",
    "\n",
    "- As for us, we will split the dataset text up into subsequences with a fixed length of 100 characters, an arbitrary length. We could just as easily split the data up by sentences and pad the shorter sequences and truncate the longer ones (but it makes more sense this way, trust me!).\n",
    "\n",
    "- Each training pattern of the network is comprised of 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole dataset one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters of course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we split up the dataset into these sequences, we convert the characters to integers using our lookup table we prepared earlier (remember the dictionaries? If you don't, scroll up!).\n",
    "- Enough talking! (for a few minutes for now...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_characters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d3ef57c5e1ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_characters\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0minput_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaned_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moutput_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaned_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_characters' is not defined"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "sequence_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, num_characters - sequence_length, 1):\n",
    "\tinput_sequence = cleaned_text[i:i + sequence_length]\n",
    "\toutput_sequence = cleaned_text[i + sequence_length]\n",
    "\tdataX.append([character_to_integer[char] for char in input_sequence])\n",
    "\tdataY.append(character_to_integer[output_sequence])\n",
    "num_patterns = len(dataX)\n",
    "print('Life is awesome right? The total number of patterns is {}'.format(num_patterns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we have prepared our training data we need to transform it so that it is suitable for use with Keras.\n",
    "\n",
    "- First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
    "\n",
    "- Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding (remember Friday's lecture?). \n",
    "- This is so that we can configure the network to predict the probability of each of the (_vocabulary length goes here_) different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. \n",
    "\n",
    "- Each y value is converted into a sparse vector (don't let this word get to you, if you don't know what it means, ask uncle Google :) ), with a length of (_vocabulary length goes here_), full of zeros except with a 1 in the column for the letter (integer) that the pattern represents.\n",
    "\n",
    "- This is implemented easily, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (num_patterns, sequence_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(vocabulary_length)\n",
    "\n",
    "# one hot encode the output variable (Remember what Doc said about this last week?)\n",
    "y = np_utils.to_categorical(dataY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, we are done with the _not really boring_ stuff, we can safely define our model based on an LSTM.\n",
    "- You might be asking, \"This is the _n_ th time you have mentioned this <b>LSTM</b> thing! What is it?\"\n",
    "- It is an acronym for <b>L</b>ong <b>S</b>hort <b>T</b>erm <b>M</b>emory. It is a type of a neural network that works by repeating a chain of the same network, each passing a message to a successor, but with a bit of memory (_I mean it's in the name isn't it?_).\n",
    "\n",
    "- Here we define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20. \n",
    "\n",
    "- The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the characters between 0 and 1.\n",
    "\n",
    "- The problem is really a single character classification problem with (_vocabulary length goes here_) classes and as such is defined as optimizing the log loss (cross entropy), here using the ADAM optimization algorithm for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]))) # this is the LSTM layer. Looks too simple right?\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The network is slow to train (especially on my machine), so we will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch. We will use the best set of weights (lowest loss) to instantiate our generative model in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "weight_filepath=\"weights/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weight_filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can now fit our model to the data (Finally! _again :)_ ) But this is the real deal. Here we use 50 epochs (Don't try to run this unless you have an <b>NVIDIA</b> graphics card on your laptop. Why? Because it might take weeks training on your <b>Intel</b> processor. Trust me, you do not want this!) and a large batch size of 128 patterns to train our baby AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=50, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After running the example, you should have a number of weight checkpoint files in the local \"weights\"  directory (if you have one! Otherwise you should get a red ugly error telling you that you don't!).\n",
    "\n",
    "- You can delete them all except the one with the smallest loss value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "weight_file = \"weights/weights-improvement-32-0.2376.hdf5\"\n",
    "model.load_weights(weight_file)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as we want to predict new characters (e.g. a sequence of 1,000 characters in length).\n",
    "\n",
    "- We can pick a random input pattern as our seed sequence, then print generated characters as we generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "starting_seed = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[starting_seed]\n",
    "print('Our seed: ')\n",
    "print(\"\\\"\", ''.join([integer_to_character[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(vocabulary_length)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = integer_to_character[index]\n",
    "\tinput_sequence = [integer_to_character[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print(\"\\nCompleted!!!.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After training, the results might not be satisfactory (imagine if you ask a baby to write your assignment/homework for you, that won't go well right?), so we can try parameter tuning etc. in order to make it better.\n",
    "- We could try to increase the size of the network, or we could change the cost function (the `categorical_crossentropy` mentioned above) or we could give ourselves a pat on the back for creating a baby AI that can write a script for some episodes of a t.v. show, I mean anyone could have done it right?\n",
    "\n",
    "### Next then what? \n",
    "- <b>Game of Thrones?</b>\n",
    "- or maybe <b>Infinity War 2 or 3?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tv script generation using AI.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
